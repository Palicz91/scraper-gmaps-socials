# ğŸ§  GMaps + Social Media Scraper (Full Pipeline)

This project automates the process of collecting business data from **Google Maps** and then enriching it with **social media and contact information** from the businessesâ€™ own websites.

---

## ğŸš€ Overview

The pipeline runs **in four stages**:

1. **Make Queries** â€“ Generates all Google Maps search queries  
2. **Search Query** â€“ Searches all queries and collects place URLs  
3. **Get Place Data** â€“ Scrapes all places for name, address, website, rating, etc.  
4. **Social Media Scraper** â€“ Enriches that data with emails & social links (Facebook, Instagram, LinkedIn, etc.)

All four scripts are automatically executed by **`run_all.py`**, in the correct order.

---

## ğŸ§© Folder Structure

20251105 Scraper/
â”œâ”€â”€ run_all.py
â”œâ”€â”€ run_all_log.txt
â”œâ”€â”€ 20251105 GMaps Scraper/
â”‚ â”œâ”€â”€ make_queries.py
â”‚ â”œâ”€â”€ search_query.py
â”‚ â”œâ”€â”€ get_place_data.py
â”‚ â”œâ”€â”€ locations.txt
â”‚ â”œâ”€â”€ brands.txt
â”‚ â”œâ”€â”€ categories.txt
â”‚ â”œâ”€â”€ google_maps_queries.txt
â”‚ â”œâ”€â”€ links.txt
â”‚ â””â”€â”€ places_data.csv
â””â”€â”€ 20251105 Socials Scraper/
â”œâ”€â”€ social_media_scraper.py
â”œâ”€â”€ input.csv
â”œâ”€â”€ output.xlsx
â””â”€â”€ scraper.log

yaml
Copy code

---

## âš™ï¸ How to Run Everything (Full Automation)

From the main folder:

```bash
cd ~/Downloads/"20251105 Scraper"
python3 run_all.py


What happens automatically:

Runs the 3 Google Maps scrapers (queries â†’ links â†’ data)

Copies places_data.csv into the Social scraper folder as input.csv (overwrites any previous input)

Launches the Social Media Scraper

Generates output.xlsx with enriched social data

If any script fails, it will retry automatically and log errors in run_all_log.txt.

ğŸª„ Part 1: Google Maps Scraper
ğŸ“ Step 1 â€“ Make Queries
Input files:

locations.txt â†’ List of cities or regions

brands.txt â†’ List of brand names (optional)

categories.txt â†’ Business categories (e.g., "restaurant", "barber")

Run manually (optional):

bash
Copy code
python make_queries.py
Output:

google_maps_queries.txt â†’ Contains all combined search queries

ğŸ” Step 2 â€“ Search Query
Reads the queries from google_maps_queries.txt, searches each on Google Maps, and saves all found business URLs.

Run manually (optional):

bash
Copy code
python search_query.py
Output:

links.txt â†’ List of all discovered business place URLs

ğŸ§¾ Step 3 â€“ Get Place Data
Reads all links from links.txt, opens each business page, and scrapes detailed data.

Run manually (optional):

bash
Copy code
python get_place_data.py
Output:

places_data.csv â†’ Contains name, category, address, rating, reviews, website, phone, etc.

âš™ï¸ Features
Headless Mode: Runs Chrome invisibly (no open browser window)

Automatic Retry: Re-attempts on network or rendering errors

Progress Resume: Saves last_processed.txt to continue after interruptions

Logging: Errors are written to scraper_log.txt

ğŸ“± Part 2: Social Media Scraper
This script enriches the places_data.csv data with contact and social media information.

ğŸ”¹ Input
Automatically created by run_all.py â€” it copies places_data.csv into this folder as input.csv.
Your CSV must include a website column.

ğŸ”¹ Run the Scraper
bash
Copy code
python social_media_scraper.py
ğŸ”¹ Output
output.xlsx â€“ Contains the original columns plus:

scraped_email

scraped_phone

scraped_whatsapp

scraped_facebook

scraped_instagram

scraped_linkedin

scraped_twitter

scraped_tiktok

ğŸ§  Features
Smart Email Prioritization (info@, contact@, hello@, etc.)

Automatic Social Detection (Facebook, Instagram, LinkedIn, TikTok, Twitter)

Phone & WhatsApp Extraction

Robust Error Handling & Logging

Incremental Save: Saves progress after each row

Runs in Headless Mode: Browser invisible by default

âš™ï¸ Installation
Install dependencies once (from the main folder):

bash
Copy code
pip install playwright pandas chardet openpyxl selenium scrapy
playwright install
Make sure Chrome or Chromium is installed on your machine.

ğŸªµ Logs and Outputs
run_all_log.txt â†’ Full pipeline log

scraper.log â†’ Social scraper logs

places_data.csv â†’ Raw GMaps output

output.xlsx â†’ Final enriched data

ğŸ§© Summary
âœ… The GMaps scraper collects all business data
âœ… The Social scraper enriches it with contact & social profiles
âœ… The run_all.py script runs the full pipeline automatically â€” start to finish

With this setup, you can collect, enrich, and export verified business intelligence data with a single command.
